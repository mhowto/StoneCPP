#define BOOST_VARIANT_MINIMIZE_SIZE

#include <boost/config/warning_disable.hpp>
//[wcp_includes
#include <boost/spirit/include/qi.hpp>
#include <boost/spirit/include/lex_lexertl.hpp>
#include <boost/spirit/include/phoenix_operator.hpp>
#include <boost/spirit/include/phoenix_statement.hpp>
#include <boost/spirit/include/phoenix_container.hpp>
//]

#include <iostream>
#include <string>
#include <stdexcept>
#include "token.h"

#include "example.h"

//[wcp_namespaces
using namespace boost::spirit;
using namespace boost::spirit::ascii;
//]

///////////////////////////////////////////////////////////////////////////////
//  Token definition: We use the lexertl based lexer engine as the underlying 
//                    lexer type.
///////////////////////////////////////////////////////////////////////////////
//[wcp_token_ids
enum tokenids
{
    IDANY = lex::min_token_id + 10
};
//]

//[wcp_token_definition
template <typename Lexer>
struct StoneLexer : lex::lexer<Lexer>
{
    StoneLexer()
    {
        // define patterns (lexer macros) to be used during token definition 
        // below
        //this->self.add_pattern("STRING", "\\\"(\\\"|\\\\\\\\|\\\\\n|[^\"])*\"");
        //this->self.add_pattern("STRING", R"("(\\|\n|[^"])*")");
        this->self.add_pattern("STRING", "\\\"(\\\n|[^\\\"])*\\\"");
        //this->self.add_pattern("STRING", R"("(\n|[^"])*")");
        this->self.add_pattern("NUMBER", "[0-9]+");
        this->self.add_pattern("IDENTIFIER", "[A-Za-z_][A-Za-z_0-9]*|==|<=|>=|&&|\\\|\\\||[!\\\"#$%&'\\)\\(\\*\\+,-\\./:;<=>\\?@\\[\\\\\\]^_`\\\|\\}\\{~]");
        this->self.add_pattern("COMMENT", "//.*");

        // define tokens and associate them with the lexer
        stone_string = "{STRING}";
        stone_number = "{NUMBER}";
        stone_id = "{IDENTIFIER}";
        stone_comment = "{COMMENT}";

        this->self.add
            (stone_string)
            (stone_number)
            (stone_id)
            (stone_comment)
            ;
    }


    // the token 'word' exposes the matched string as its parser attribute
    lex::token_def<> stone_string, stone_number, stone_id, stone_comment;
    std::string file_name;
};
//]

///////////////////////////////////////////////////////////////////////////////
//  Grammar definition
///////////////////////////////////////////////////////////////////////////////
//[wcp_grammar_definition
template <typename Iterator>
struct StoneGrammar : qi::grammar<Iterator>
{
    template <typename TokenDef>
    StoneGrammar(TokenDef const& tok)
        : StoneGrammar::base_type(start)
        , c(0), w(0), l(0)
    {
        using boost::phoenix::ref;
        using boost::phoenix::size;
        using boost::phoenix::push_back;

        start =
            //*(tok.stone_string[tokens.push_back(std::make_unique<StrToken>("file_name", 1, 2, lex::_val))]
            *(tok.stone_string[++ref(c)]
            //*(tok.stone_string[push_back(tokens, std::make_unique<StrToken>("file_name", 1, 2, "sttt"))]
            | tok.stone_number[++ref(c), ++ref(l)]
            | tok.stone_id[++ref(c)]
            | tok.stone_comment
            )
            ;
    }

    std::size_t c, w, l;
    std::vector<std::unique_ptr<Token> > tokens;
    qi::rule<Iterator> start;
};
//]

///////////////////////////////////////////////////////////////////////////////
//[wcp_main
int main(int argc, char* argv[])
{
    /*<  Define the token type to be used: `std::string` is available as the type of the token attribute
    >*/
    typedef lex::lexertl::token< char const*, boost::mpl::vector<std::string> > token_type;



    /*<  Define the lexer type to use implementing the state machine
    >*/  typedef lex::lexertl::lexer<token_type> lexer_type;

    /*<  Define the iterator type exposed by the lexer type
    >*/  typedef StoneLexer<lexer_type>::iterator_type iterator_type;

    // now we use the types defined above to create the lexer and grammar
    // object instances needed to invoke the parsing process
    StoneLexer<lexer_type> stone_lexer;          // Our lexer
    StoneGrammar<iterator_type> g(stone_lexer);  // Our parser 

                                              // read in the file int memory
    std::string str(read_from_file(1 == argc ? "while_state.input" : argv[1]));
    char const* first = str.c_str();
    char const* last = &first[str.size()];

    /*<  Parsing is done based on the token stream, not the character
    stream read from the input. The function `tokenize_and_parse()` wraps
    the passed iterator range `[first, last)` by the lexical analyzer and
    uses its exposed iterators to parse the token stream.
    >*/  
    bool r;
    try {
        r = lex::tokenize_and_parse(first, last, stone_lexer, g);
    
    }
    catch (std::runtime_error& err) {
        std::cout << err.what();
        goto EXIT;
    }
    char s;
    std::cin >> s;

    if (r) {
        std::cout << "lines: " << g.l << ", words: " << g.w
            << ", characters: " << g.c << "\n";
    }
    else {
        std::string rest(first, last);
        std::cerr << "Parsing failed\n" << "stopped at: \""
            << rest << "\"\n";
    }
EXIT:
    std::cin >> s;
    return 0;
}
//]
